services:
  vllm-2b: &vllm-base
    image: vllm/vllm-openai:latest
    container_name: notevlm-vllm-2b
    profiles:
      - vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["0"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >-
      --model Qwen/Qwen3-VL-2B-Instruct
      --served-model-name Qwen/Qwen3-VL-2B-Instruct
      --trust-remote-code
      --gpu-memory-utilization 0.4
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    env_file:
      - .env.backend
    volumes:
      - hf-cache:/root/.cache/huggingface
    ports:
      - "8201:8000"
    restart: unless-stopped

  vllm-4b:
    <<: *vllm-base
    container_name: notevlm-vllm-4b
    profiles:
      - vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["1"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >-
      --model Qwen/Qwen3-VL-4B-Instruct
      --served-model-name Qwen/Qwen3-VL-4B-Instruct
      --trust-remote-code
      --gpu-memory-utilization 0.6
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    ports:
      - "8202:8000"

  vllm-2b-fp8:
    <<: *vllm-base
    container_name: notevlm-vllm-2b-fp8
    profiles:
      - vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["0"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >-
      --model Qwen/Qwen3-VL-2B-Instruct-FP8
      --served-model-name Qwen/Qwen3-VL-2B-Instruct-FP8
      --trust-remote-code
      --quantization fp8
      --gpu-memory-utilization 0.4
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    ports:
      - "8203:8000"

  vllm-4b-fp8:
    <<: *vllm-base
    container_name: notevlm-vllm-4b-fp8
    profiles:
      - vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["1"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >-
      --model Qwen/Qwen3-VL-4B-Instruct-FP8
      --served-model-name Qwen/Qwen3-VL-4B-Instruct-FP8
      --trust-remote-code
      --quantization fp8
      --gpu-memory-utilization 0.6
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    ports:
      - "8204:8000"

  vllm-8b:
    <<: *vllm-base
    container_name: notevlm-vllm-8b
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids:
                - "0"
                - "1"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    profiles:
      - vllm
    command: >-
      --model Qwen/Qwen3-VL-8B-Instruct
      --served-model-name Qwen/Qwen3-VL-8B-Instruct
      --trust-remote-code
      --gpu-memory-utilization 0.5
      --tensor-parallel-size 2
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    ports:
      - "8205:8000"

  vllm-8b-fp8:
    <<: *vllm-base
    container_name: notevlm-vllm-8b-fp8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids:
                - "0"
                - "1"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    profiles:
      - vllm
    command: >-
      --model Qwen/Qwen3-VL-8B-Instruct-FP8
      --served-model-name Qwen/Qwen3-VL-8B-Instruct-FP8
      --trust-remote-code
      --quantization fp8
      --gpu-memory-utilization 0.5
      --tensor-parallel-size 2
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    ports:
      - "8206:8000"

  vllm-32b:
    <<: *vllm-base
    container_name: notevlm-vllm-32b
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids:
                - "0"
                - "1"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    profiles:
      - qwen32b
    command: >-
      --model Qwen/Qwen3-VL-32B-Instruct
      --served-model-name Qwen/Qwen3-VL-32B-Instruct
      --trust-remote-code
      --tensor-parallel-size 2
      --host 0.0.0.0
      --port 8000
    ports:
      - "8207:8000"

  vllm-32b-fp8:
    <<: *vllm-base
    container_name: notevlm-vllm-32b-fp8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids:
                - "0"
                - "1"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    profiles:
      - qwen32b
    command: >-
      --model Qwen/Qwen3-VL-32B-Instruct-FP8
      --served-model-name Qwen/Qwen3-VL-32B-Instruct-FP8
      --trust-remote-code
      --quantization fp8
      --tensor-parallel-size 2
      --host 0.0.0.0
      --port 8000
    ports:
      - "8208:8000"
  deepseek-ocr:
    image: vllm/vllm-openai:nightly
    container_name: notevlm-deepseek-ocr
    profiles:
      - deepseek
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["1"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >-
      --model deepseek-ai/DeepSeek-OCR
      --served-model-name Deepseek/DeepSeek-OCR
      --trust-remote-code
      --gpu-memory-utilization 0.6
      --max-model-len 8192
      --host 0.0.0.0
      --port 8000
    env_file:
      - .env.backend
    volumes:
      - hf-cache:/root/.cache/huggingface
    ports:
      - "8209:8000"
    restart: unless-stopped
  chandra-ocr:
    image: vllm/vllm-openai:latest
    container_name: notevlm-chandra-ocr
    profiles:
      - chandra
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["1"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >-
      --model datalab-to/chandra
      --served-model-name datalab-to/chandra
      --trust-remote-code
      --gpu-memory-utilization 0.75
      --max-model-len 65536
      --host 0.0.0.0
      --port 8000
    env_file:
      - .env.backend
    volumes:
      - hf-cache:/root/.cache/huggingface
    ports:
      - "8210:8000"
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    network_mode: "host"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    env_file:
      - .env.backend
    volumes:
      - ./storage:/app/storage
      - /var/run/docker.sock:/var/run/docker.sock
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    network_mode: "host"
    environment:
      - NEXT_PUBLIC_API_URL=http://192.168.29.16:8003
      - PORT=3003
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  hf-cache:
